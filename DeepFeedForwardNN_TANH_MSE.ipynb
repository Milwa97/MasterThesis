{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = calculate_mean_and_std_FMNIST()\n",
    "train_data, test_data =  download_normalized_data_FMNIST(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 1000\n",
    "\n",
    "# prepare data loaders, based on the already loaded datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple D-layer linear neural network \n",
    "    hidden_dims = topule(n0, n1, n2, ...nD)\n",
    "    n0 = input layer\n",
    "    n_D = output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, D, layers_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize network's weights according to Gaussian iid and network's biases with 0.0 values\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self.D = len(layers_dim)-1\n",
    "        assert self.D == D\n",
    "        \n",
    "        print(\"Depth of the network = number of hidden layers + 1:\", D)\n",
    "        \n",
    "        for i in range(self.D):\n",
    "            \n",
    "            weight: torch.Tensor = torch.rand((layers_dim[i+1], layers_dim[i])) \n",
    "            bias: torch.Tensor = torch.zeros(layers_dim[i+1])  \n",
    "            \n",
    "            stdv = 2. / np.sqrt(layers_dim[i])\n",
    "            \n",
    "            weight = (weight-0.5)*stdv\n",
    "\n",
    "            weight.requires_grad = True\n",
    "            bias.requires_grad = True\n",
    "            \n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "       \n",
    "            \n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(0,self.D):            \n",
    "            x = torch.nn.functional.linear( input = x, weight=self.weights[i],bias=self.biases[i])\n",
    "            x=torch.tanh(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    \n",
    "    def parameters(self) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns all trainable parameters \n",
    "        \"\"\"\n",
    "        return self.weights+self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of the network = number of hidden layers + 1: 5\n"
     ]
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 10\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 = 50\n",
    "hidden_dim3 = 50\n",
    "hidden_dim4 = 50\n",
    "D = 5\n",
    "\n",
    "# initialize the model\n",
    "model: CustomNetwork = CustomNetwork(D = D, layers_dim = (input_dim, hidden_dim1, hidden_dim2, hidden_dim3,hidden_dim4, output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the optimizer using the hyperparams below\n",
    "lr: float = 0.005\n",
    "momentum: float = 0.00\n",
    "optimizer: torch.optim.Optimizer = SGD(params = model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epoch: int = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t train accuracy 0.2946 \tTest accuracy: 0.2922\n",
      "1 \t train accuracy 0.39093333333333335 \tTest accuracy: 0.3867\n",
      "2 \t train accuracy 0.45413333333333333 \tTest accuracy: 0.4505\n",
      "3 \t train accuracy 0.48973333333333335 \tTest accuracy: 0.4832\n",
      "4 \t train accuracy 0.5040666666666667 \tTest accuracy: 0.4972\n",
      "5 \t train accuracy 0.5105833333333333 \tTest accuracy: 0.5077\n",
      "6 \t train accuracy 0.5155 \tTest accuracy: 0.514\n",
      "7 \t train accuracy 0.5198833333333334 \tTest accuracy: 0.5174\n",
      "8 \t train accuracy 0.524 \tTest accuracy: 0.5224\n",
      "9 \t train accuracy 0.5285 \tTest accuracy: 0.5243\n",
      "10 \t train accuracy 0.53175 \tTest accuracy: 0.5308\n",
      "11 \t train accuracy 0.5355166666666666 \tTest accuracy: 0.5338\n",
      "12 \t train accuracy 0.54 \tTest accuracy: 0.5408\n",
      "13 \t train accuracy 0.5436833333333333 \tTest accuracy: 0.544\n",
      "14 \t train accuracy 0.5487166666666666 \tTest accuracy: 0.5469\n",
      "15 \t train accuracy 0.5545 \tTest accuracy: 0.5506\n",
      "16 \t train accuracy 0.5620833333333334 \tTest accuracy: 0.5604\n",
      "17 \t train accuracy 0.5721833333333334 \tTest accuracy: 0.5674\n",
      "18 \t train accuracy 0.5814666666666667 \tTest accuracy: 0.5772\n",
      "19 \t train accuracy 0.5886666666666667 \tTest accuracy: 0.5833\n",
      "20 \t train accuracy 0.5928 \tTest accuracy: 0.5871\n",
      "21 \t train accuracy 0.5972333333333333 \tTest accuracy: 0.5888\n",
      "22 \t train accuracy 0.5991 \tTest accuracy: 0.5911\n",
      "23 \t train accuracy 0.6014 \tTest accuracy: 0.5928\n",
      "24 \t train accuracy 0.6031 \tTest accuracy: 0.5945\n",
      "25 \t train accuracy 0.6047333333333333 \tTest accuracy: 0.5952\n",
      "26 \t train accuracy 0.6068166666666667 \tTest accuracy: 0.5977\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):   \n",
    "    \n",
    "    for inner_counter, (x, y) in enumerate(train_loader):        \n",
    "        \n",
    "        # reset the gradients from previouis iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pass through the network\n",
    "        \n",
    "        output: torch.Tensor = model(x)  \n",
    "        y=F.one_hot(y, num_classes=10).float()\n",
    "\n",
    "        \n",
    "        loss: torch.Tensor = criterion(output, y)\n",
    "\n",
    "        # backward pass thorught the network\n",
    "        loss.backward()\n",
    "\n",
    "        # apply the gradients\n",
    "        optimizer.step()     \n",
    "        \n",
    "    # add model parameters to tab\n",
    "    loss_tab.append(loss.item())\n",
    "    params_tab = []\n",
    "    for param in model.parameters():\n",
    "        params_tab.append(param.detach().numpy().flatten()  )\n",
    "            \n",
    "    w1_tab.append(params_tab[0])\n",
    "    w2_tab.append(params_tab[1])\n",
    "    w3_tab.append(params_tab[2])\n",
    "    w4_tab.append(params_tab[3])\n",
    "    w5_tab.append(params_tab[4])\n",
    "    b1_tab.append(params_tab[5])\n",
    "    b2_tab.append(params_tab[6])\n",
    "    b3_tab.append(params_tab[7])\n",
    "    b4_tab.append(params_tab[8])\n",
    "    b5_tab.append(params_tab[9])\n",
    "    \n",
    "    w1_mean_tab.append(params_tab[0].mean())\n",
    "    w2_mean_tab.append(params_tab[1].mean())\n",
    "    w3_mean_tab.append(params_tab[2].mean())\n",
    "    w4_mean_tab.append(params_tab[3].mean())\n",
    "    w5_mean_tab.append(params_tab[4].mean())\n",
    "    \n",
    "    b1_mean_tab.append(params_tab[5].mean())\n",
    "    b2_mean_tab.append(params_tab[6].mean())\n",
    "    b3_mean_tab.append(params_tab[7].mean())\n",
    "    b4_mean_tab.append(params_tab[8].mean())\n",
    "    b5_mean_tab.append(params_tab[9].mean())\n",
    "    \n",
    "    w1_std_tab.append(params_tab[0].std())\n",
    "    w2_std_tab.append(params_tab[1].std())\n",
    "    w3_std_tab.append(params_tab[2].std())\n",
    "    w4_std_tab.append(params_tab[3].std())\n",
    "    w5_std_tab.append(params_tab[4].std())\n",
    "\n",
    "    b1_std_tab.append(params_tab[5].std())\n",
    "    b2_std_tab.append(params_tab[6].std())\n",
    "    b3_std_tab.append(params_tab[7].std())\n",
    "    b4_std_tab.append(params_tab[8].std())\n",
    "    b5_std_tab.append(params_tab[9].std())\n",
    "    \n",
    "    # at the end of an epoch run evaluation on the test set\n",
    "    with torch.no_grad():\n",
    "        correct: int = 0 \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # pass through the network            \n",
    "            output: torch.Tensor = model(x)\n",
    "            # update the number of correctly predicted examples\n",
    "            pred_labels = torch.argmax(output, 1)\n",
    "            correct += ( (pred_labels -y) ==0).sum()\n",
    "        train_accuracy_tab.append( float(correct) / len(train_data))\n",
    "\n",
    "        correct: int = 0 \n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            # pass through the network            \n",
    "            output: torch.Tensor = model(x)\n",
    "\n",
    "            # update the number of correctly predicted examples\n",
    "            pred_labels = torch.argmax(output, 1)\n",
    "            correct += ( (pred_labels -y) ==0).sum()\n",
    "        test_accuracy_tab.append( float(correct) / len(test_data))\n",
    "            \n",
    "    print(f\"{e} \\t train accuracy {train_accuracy_tab[-1]} \\tTest accuracy: {test_accuracy_tab[-1]}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (16, 5))\n",
    "\n",
    "axs[0].plot(loss_tab)\n",
    "axs[1].plot(train_accuracy_tab, color='green')\n",
    "axs[1].plot(test_accuracy_tab, color='red')\n",
    "\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('accuracy')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (16, 5))\n",
    "\n",
    "axs[0].plot(w1_mean_tab, c='red', label = 'w1 mean')\n",
    "axs[0].plot(w2_mean_tab, c='blue',label = 'w2 mean')\n",
    "axs[0].plot(w3_mean_tab, c='green', label = 'w3 mean')\n",
    "axs[0].plot(w4_mean_tab, c='purple', label = 'w4 mean')\n",
    "#axs[0].plot(w5_mean_tab, c='pink', label = 'w5 mean')\n",
    "\n",
    "\n",
    "axs[1].plot(w1_std_tab, color='red', label='w1 std')\n",
    "axs[1].plot(w2_std_tab, color='blue', label='w2 std')\n",
    "axs[1].plot(w3_std_tab, color='green', label='w3 std')\n",
    "axs[1].plot(w4_std_tab, color='purple', label='w4 std')\n",
    "#axs[1].plot(w5_std_tab, color='pink', label='w5 std')\n",
    "\n",
    "\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].set_ylabel('w mean')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('w std')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save generat time evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "savefile = 'model_50_50_50_50_3.csv'\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['loss'] = loss_tab\n",
    "df['train_accuracy'] = train_accuracy_tab\n",
    "df['test_accuracy'] = test_accuracy_tab\n",
    "\n",
    "df['w1_mean'] = w1_mean_tab\n",
    "df['w1_std'] = w1_std_tab\n",
    "df['w2_mean'] = w2_mean_tab\n",
    "df['w2_std'] = w2_std_tab\n",
    "df['w3_mean'] = w3_mean_tab\n",
    "df['w3_std'] = w3_std_tab\n",
    "df['w4_mean'] = w4_mean_tab\n",
    "df['w4_std'] = w4_std_tab\n",
    "df['w5_mean'] = w5_mean_tab\n",
    "df['w5_std'] = w5_std_tab\n",
    "\n",
    "\n",
    "df['b1_mean'] = b1_mean_tab\n",
    "df['b1_std'] = b1_std_tab\n",
    "df['b2_mean'] = b2_mean_tab\n",
    "df['b2_std'] = b2_std_tab\n",
    "df['b3_mean'] = b3_mean_tab\n",
    "df['b3_std'] = b3_std_tab\n",
    "df['b4_mean'] = b4_mean_tab\n",
    "df['b4_std'] = b4_std_tab\n",
    "df['b5_mean'] = b5_mean_tab\n",
    "df['b5_std'] = b5_std_tab\n",
    "\n",
    "\n",
    "df.to_csv(savefile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2= pd.DataFrame(np.array(w2_tab).T)\n",
    "df_w3= pd.DataFrame(np.array(w3_tab).T)\n",
    "df_w4= pd.DataFrame(np.array(w4_tab).T)\n",
    "df_w5= pd.DataFrame(np.array(w5_tab).T)\n",
    "\n",
    "df_b2= pd.DataFrame(np.array(b2_tab).T)\n",
    "df_b3= pd.DataFrame(np.array(b3_tab).T)\n",
    "df_b4= pd.DataFrame(np.array(b4_tab).T)\n",
    "df_b5= pd.DataFrame(np.array(b5_tab).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w2.to_csv('model_50_50_50_50_w2_3.csv', index=False)\n",
    "df_w3.to_csv('model_50_50_50_50_w3_3.csv', index=False)\n",
    "df_w4.to_csv('model_50_50_50_50_w4_3.csv', index=False)\n",
    "df_w5.to_csv('model_50_50_50_50_w5_3.csv', index=False)\n",
    "\n",
    "df_b2.to_csv('model_50_50_50_50_b2_3.csv', index=False)\n",
    "df_b3.to_csv('model_50_50_50_50_b3_3.csv', index=False)\n",
    "df_b4.to_csv('model_50_50_50_50_b4_3.csv', index=False)\n",
    "df_b5.to_csv('model_50_50_50_50_b5_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=deepcopy(model.parameters()[0]).detach().numpy().reshape(input_dim*hidden_dim1)\n",
    "w2=deepcopy(model.parameters()[1]).detach().numpy().reshape(hidden_dim1*hidden_dim2)\n",
    "w3=deepcopy(model.parameters()[2]).detach().numpy().reshape(hidden_dim2*hidden_dim3)\n",
    "w4=deepcopy(model.parameters()[3]).detach().numpy().reshape(hidden_dim3*hidden_dim4)\n",
    "w5=deepcopy(model.parameters()[4]).detach().numpy().reshape(hidden_dim4*output_dim)\n",
    "\n",
    "b1=deepcopy(model.parameters()[5]).detach().numpy()\n",
    "b2=deepcopy(model.parameters()[6]).detach().numpy()\n",
    "b3=deepcopy(model.parameters()[7]).detach().numpy()\n",
    "b4=deepcopy(model.parameters()[8]).detach().numpy()\n",
    "b5=deepcopy(model.parameters()[9]).detach().numpy()\n",
    "\n",
    "\n",
    "df_w1 = pd.DataFrame(w1, columns = ['w1'] )\n",
    "df_w2 = pd.DataFrame(w2, columns = ['w2'] )\n",
    "df_w3 = pd.DataFrame(w3, columns = ['w3'] )\n",
    "df_w4 = pd.DataFrame(w4, columns = ['w4'] )\n",
    "df_w5 = pd.DataFrame(w5, columns = ['w5'] )\n",
    "\n",
    "\n",
    "df_b1 = pd.DataFrame(b1, columns = ['b1'] )\n",
    "df_b2 = pd.DataFrame(b2, columns = ['b2'] )\n",
    "df_b3 = pd.DataFrame(b3, columns = ['b3'] )\n",
    "df_b4 = pd.DataFrame(b4, columns = ['b4'] )\n",
    "df_b5 = pd.DataFrame(b5, columns = ['b5'] )\n",
    "\n",
    "\n",
    "df_w1.to_csv('w1.csv', index=False)\n",
    "df_w2.to_csv('w2.csv', index=False)\n",
    "df_w3.to_csv('w3.csv', index=False)\n",
    "df_w4.to_csv('w4.csv', index=False)\n",
    "df_w5.to_csv('w5.csv', index=False)\n",
    "\n",
    "df_b1.to_csv('b1.csv', index=False)\n",
    "df_b2.to_csv('b2.csv', index=False)\n",
    "df_b3.to_csv('b3.csv', index=False)\n",
    "df_b4.to_csv('b4.csv', index=False)\n",
    "df_b5.to_csv('b5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
